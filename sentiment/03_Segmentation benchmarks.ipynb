{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef94e781",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sulverus/dev/asr/sentiment/env/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchtext.legacy import data\n",
    "import random\n",
    "from torchtext.legacy import datasets\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af61e359",
   "metadata": {},
   "source": [
    "### Experiment\n",
    "In this notebook I will compare different approaches for segmentation in order to get the best score\n",
    "TO do this I prepares following experiment:\n",
    "1. Freeze the sentiment model\n",
    "2. Implement multiple text preprocessing pipelines\n",
    "3. Calculate Acuracy score with frozen models and different pipelines\n",
    "4. Get the best result for web service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ff5f12",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "Here we will load 1000 samples from the full dataset and use them as a benchmark data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2e4f3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = data.Field(tokenize = 'spacy',\n",
    "                  tokenizer_language = 'en_core_web_sm',\n",
    "                  include_lengths = True)\n",
    "labels = data.LabelField(dtype = torch.float)\n",
    "\n",
    "train_data, test_data = datasets.IMDB.splits(texts, labels)\n",
    "_, valid_data = train_data.split(random_state = random.seed(SEED))\n",
    "validation = [(' '.join(v.text), 1 if v.label=='pos' else 0) for v in valid_data]\n",
    "positive_tests = []\n",
    "negative_tests = []\n",
    "for (text, label) in validation:\n",
    "    if label == 1 and len(positive_tests) < 500:\n",
    "        positive_tests.append((text, label))\n",
    "    if label == 0 and len(negative_tests) < 500:\n",
    "        negative_tests.append((text, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f620d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "# No need for balance check, because we manually selecting 500/500 rows\n",
    "print(len(positive_tests))\n",
    "print(len(negative_tests))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "777bb876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_validation = positive_tests + negative_tests\n",
    "len(balanced_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d933e5",
   "metadata": {},
   "source": [
    "### Load trained model\n",
    "Here I will load the model from previous notebook to do this I will need do following steps:\n",
    "* Load pickled embeddings\n",
    "* Define the model class (copy/paste from previous notebook)\n",
    "* Load trained model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c85dae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('embeddings.pickle', 'rb') as f:\n",
    "    TEXT = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0069509",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
    "                 bidirectional, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        self.rnn = nn.LSTM(embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, text, text_lengths):\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'))\n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90c7ea02",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 16#256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = RNN(INPUT_DIM, \n",
    "            EMBEDDING_DIM, \n",
    "            HIDDEN_DIM, \n",
    "            OUTPUT_DIM, \n",
    "            N_LAYERS, \n",
    "            BIDIRECTIONAL, \n",
    "            DROPOUT, \n",
    "            PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b544847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_OUTPUT = 'sentiment_lstm_glove.pt'\n",
    "model.load_state_dict(torch.load(MODEL_OUTPUT))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be305c7",
   "metadata": {},
   "source": [
    "### Benchmarks\n",
    "For benchmarks I prepared modified version of `predict_sentiment` function. It will get additional argument for processing function. In addition we have to switch device to \"CPU\" because we should not run the production inference on GPU (to save resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94cf5e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE_NAME = 'cpu'\n",
    "device = torch.device(DEVICE_NAME)\n",
    "\n",
    "def predict_sentiment(model, sentence, proc_func):\n",
    "    model.eval()\n",
    "    \n",
    "    tokenized = proc_func(sentence)\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    length = [len(indexed)]\n",
    "    \n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    length_tensor = torch.LongTensor(length)\n",
    "    prediction = torch.sigmoid(model(tensor, length_tensor))\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6fd1d0",
   "metadata": {},
   "source": [
    "# Methods\n",
    "First 2 methods are easy: naive version with regular expression and intermediate version with spacy tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea42c48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def naive_preprocessing(text):\n",
    "    st = re.sub(r'[^A-Za-z0-9 ]+', '', text)\n",
    "    tokens = st.split(' ')\n",
    "    return tokens\n",
    "\n",
    "def spacy_tokenizer(text):\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(text)]\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5d6d7dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7301585674285889"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " predict_sentiment(model, \"This film is great\", naive_preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a48941",
   "metadata": {},
   "source": [
    "### Segmentation\n",
    "Advanced approach is to use segmentation first, split the text into multiple utterances and run inference for each utterance. Then we can calculate mean value and consider it as final result. \n",
    "\n",
    "Remark:\n",
    "As additional idea we can train extra model in order to return final score (based on all sentenses) but it's not implemented here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "288a65e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmented_sentiment(text, proc_func):\n",
    "    data = nlp(text)\n",
    "    scores = []\n",
    "    for sent in data.sents:\n",
    "        scores.append(predict_sentiment(model, str(sent), proc_func))\n",
    "    return round(sum(scores) / float(len(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfb3520",
   "metadata": {},
   "source": [
    "### Benchmark\n",
    "For benchmark we have 4 pipelines:\n",
    "* naive approach with regular expression\n",
    "* spacy tokenisation for all text\n",
    "* segmentation + regular expression + avg\n",
    "* segmentation + spacy tokenisation + avg\n",
    "\n",
    "The best result will be used in the web service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c084ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "def benchmark_tokenisation():\n",
    "    for func in [naive_preprocessing, spacy_tokenizer]:\n",
    "        labels = [v[1] for v in balanced_validation]\n",
    "        preds = [round(predict_sentiment(model, v[0], func)) for v in balanced_validation]\n",
    "        print(func.__name__, accuracy_score(labels, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "59baa756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive_preprocessing 0.81\n",
      "spacy_tokenizer 0.81\n"
     ]
    }
   ],
   "source": [
    "benchmark_tokenisation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "db0ba12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_segmentation():\n",
    "    for func in [naive_preprocessing, spacy_tokenizer]:\n",
    "        labels = [v[1] for v in balanced_validation]\n",
    "        preds = [round(segmented_sentiment(v[0], func)) for v in balanced_validation]\n",
    "        print(\"segment + \" + func.__name__, accuracy_score(labels, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "15796a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segment + naive_preprocessing 0.798\n",
      "segment + spacy_tokenizer 0.831\n"
     ]
    }
   ],
   "source": [
    "benchmark_segmentation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5f0162",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "As we can see, segmentation + spacy tokenization + avg value is the better then other solutions (we won ~2% of accuracy only with preprocessing pipeline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
