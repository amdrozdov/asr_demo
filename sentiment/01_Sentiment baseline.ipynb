{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa9be31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torchtext.legacy import data\n",
    "from torchtext.legacy import datasets\n",
    "import random\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2d69a7",
   "metadata": {},
   "source": [
    "### Basic tokenisation with spacy\n",
    "In order to process original text we will use spacy default en model as tokenizer (instead of using default string.split function) and will use vocabulary to collect a set of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f46174f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED=1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "TEXT = data.Field(tokenize = 'spacy', tokenizer_language = 'en_core_web_sm')\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faecf4d8",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "For demonstration I will use IMDB movie review dataset, which is available in torchtext library\n",
    "and it was used for sentiment analysis in many publications.\n",
    "In order to check overfitting we have to split dataset into 3 parts:\n",
    "* training set - will be used for training of ANN\n",
    "* validation set - will be used for validation during training\n",
    "* test set - hidden data set in order to check the model after training process in separate data (this data was not used for training and validation)\n",
    "\n",
    "### Strategy\n",
    "Our goal is to get the prediction score as a continious value from -1 to 1, where -1 is negative, 0 is neutral and 1 is positive. In this dataset we have only negative and positive lables. But we can train the model as binary classifier that will return 0 for negative and 1 for positive sentiment. We can map the output of the ANN to the interval(-1;1). Alternative solution is to train multiclass classifier and use probability of each class in order to get the final score. But binary classifier is good solution for this dataset(we don't have neutarl lables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c71ba952",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "train_data, valid_data = train_data.split(random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6193d845",
   "metadata": {},
   "source": [
    "### Dataset lables sanity check\n",
    "Here we can see that ratio for positive and negative labels for each dataset is around 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a194e532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set has 8690/8810 positive/negative samples balance=0.99\n",
      "test set has 12500/12500 positive/negative samples balance=1.00\n",
      "valid set has 3810/3690 positive/negative samples balance=1.03\n"
     ]
    }
   ],
   "source": [
    "def check_balance(ds, name):\n",
    "    positive = 0\n",
    "    negative = 0\n",
    "    for row in ds:\n",
    "        if row.label == 'pos':\n",
    "            positive += 1\n",
    "        else:\n",
    "            negative += 1\n",
    "    print(name + \" has %s/%s positive/negative samples\" % (positive, negative), 'balance=%.02f' % float(positive/negative))\n",
    "    \n",
    "check_balance(train_data, \"train set\")\n",
    "check_balance(test_data, \"test set\")\n",
    "check_balance(valid_data, \"valid set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f19b30",
   "metadata": {},
   "source": [
    "### Vocabulary\n",
    "For the baseline I will use default vocabulary based on default torch embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a50888d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25_000\n",
    "\n",
    "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42645e06",
   "metadata": {},
   "source": [
    "### Create training pipeline\n",
    "In order to train our model I will create iterators based on train, test and validation datasets\n",
    "In addition I will try to train all models with Cuda on my old nVidia card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b07e5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d768e74",
   "metadata": {},
   "source": [
    "### Model\n",
    "Here we will have 3 layers:\n",
    "* embedding as input\n",
    "* default recurrent neural network\n",
    "* Linear layer as hidden layer+output layer\n",
    "\n",
    "Important:\n",
    "Unfortunatly my local PC is a bit old and it will take tonns of time to train really good model, I was trying to artificially prune/simplify the model in order to debug the pipeline and get some results in less then 5-6 hours.\n",
    "You can see that hidden layer size is reduced to 16 (but in powerfull PR it can be 128, 256 or even more)\n",
    "\n",
    "Remark:\n",
    "If i set hidden layer size to 256 - i will get around 4.5mln parameters, the result of prunning/simplification is 2.5 mln parameters\n",
    "\n",
    "Remark2:\n",
    "I was trying to run bigger hidden layer size, but I was always getting cuda memory errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30f4d69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n",
    "        return self.fc(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80071848",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 16\n",
    "OUTPUT_DIM = 1\n",
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2398ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable params=2502105\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Total trainable params={total_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850c0e1c",
   "metadata": {},
   "source": [
    "### Training\n",
    "For baseline I used SGD optimiser and binary cross entropy loss function with sigmoid layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d320a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8691e8",
   "metadata": {},
   "source": [
    "### Training functions\n",
    "In order to do training and validation I will ned some auxilarity functions\n",
    "* accuracy calculation\n",
    "* training loop\n",
    "* evaluation loop (which will use accuracy function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26db3eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6112ef5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd6c68f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            loss = criterion(predictions, batch.label)\n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39cf473",
   "metadata": {},
   "source": [
    "### Main training loop\n",
    "Here you can see limited amount of epochs(5) and cuda memory limitation (500mb) in order to be able to get a model in a reasonable amount of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0da7f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.694 | Train Acc: 49.58%\n",
      "\t Val. Loss: 0.697 |  Val. Acc: 49.94%\n",
      "Epoch: 02 | Epoch Time: 0m 17s\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.23%\n",
      "\t Val. Loss: 0.697 |  Val. Acc: 49.10%\n",
      "Epoch: 03 | Epoch Time: 0m 17s\n",
      "\tTrain Loss: 0.694 | Train Acc: 50.27%\n",
      "\t Val. Loss: 0.697 |  Val. Acc: 49.00%\n",
      "Epoch: 04 | Epoch Time: 0m 17s\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.38%\n",
      "\t Val. Loss: 0.696 |  Val. Acc: 49.07%\n",
      "Epoch: 05 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.47%\n",
      "\t Val. Loss: 0.696 |  Val. Acc: 49.16%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:500\"\n",
    "MODEL_OUTPUT = 'sentiment_baseline.pt'\n",
    "N_EPOCHS = 5\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), MODEL_OUTPUT)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {elapsed_mins}m {elapsed_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1353af9",
   "metadata": {},
   "source": [
    "### Final results\n",
    "In the end I can load the model and evalueate it on hidden dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e28d7e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.678 | Test Acc: 59.29%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(MODEL_OUTPUT))\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1e41a6",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "We got 59% of accuracy, which is acceptable for first attempt, but very bad for final results. I will use this \"boilerplate\" for multiple experiments. In addition we can see that loos function is not decreasing a lot, so we need to work on data quality and maybe embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
